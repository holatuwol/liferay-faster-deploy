{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 2: Visualizing Ticket Load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we start with the following research question. \"How can we create data visualizations on top of the LESA, LPS, LPP, and BPR ticket metadata that lets us group together different tickets so that we can explore the times that tickets remain in each status based on those groupings?\"\n",
    "\n",
    "In order to investigate the answer to this question, we start with a much smaller sub-question that focuses on more recent data.\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "In order to answer this question, this notebook explores a few concepts in databases related to metadata and deriving additional statistics on top of that metadata.\n",
    "\n",
    "At the end of this notebook, we will have a script that takes a sample of data from JIRA and enriches it with more data from JIRA, and the reader will have an improved understanding of the capabilities databases provide when it comes to processing information for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y matplotlib scikit-learn seaborn statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from checklpp import *\n",
    "from datetime import datetime, timedelta\n",
    "import functools\n",
    "from IPython.core.display import display, HTML\n",
    "import matplotlib\n",
    "from multiprocessing import Pool, cpu_count\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enable Process Pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our computations can run independently of each other, so let's take advantage of some parallelization that's available on our machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pool = Pool(cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also change some of the default plots so that they're larger and use a white background instead of a gray one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = (6.4*2, 4.8*1.5)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also change some of our default colors to make them a little more color-blind friendly than the default plotting colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_palette('colorblind')\n",
    "sns.palplot(sns.color_palette('colorblind'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Database Views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you work at Liferay, it is easy to be lead to believe that databases exist as naive information storage and retrieval systems, because that is really all Liferay does with a database. However, the truth is much more complex.\n",
    "\n",
    "One of the other reasons databases exist is to manage relationships between data sets and allow you to analyze those relationships. As a result, many enterprise database vendors have created a rich set of proprietary functions on top of SQL that allow you to perform very insightful analysis.\n",
    "\n",
    "Because you're uncovering relationships within the data, whenever you want to answer questions, you often join together tables and then perform additional analysis on top of those joined tables. It is not uncommon for an analysis to simply take multiple sets of SQL used to generate join tables and use them as subqueries.\n",
    "\n",
    "Over time, it can be tedious to constantly paste in the the same nested queries over and over again, and having too many of them will also make it harder for anyone reading the query to understand what it is you were trying to analyze. To address this problem, it is common to create a database view.\n",
    "\n",
    "* [Why do you create a view in a database?](https://stackoverflow.com/questions/1278521/why-do-you-create-a-view-in-a-database)\n",
    "\n",
    "You can think of a database view as an alias for a query result that someone has needed before. We also know that every query result is a table, and knowing why people want that information to begin with (constantly reusing it in different analyses), you can also see why people might like a feature that allows you to materialize those views.\n",
    "\n",
    "* [Materialized view](https://en.wikipedia.org/wiki/Materialized_view)\n",
    "\n",
    "In our case, whenever we fetch tickets from JIRA, the JSON object may be cumbersome to work with. Therefore, in order to make it easier for people to understand the underlying information, we will create a view on top of it that allows us to concentrate on the specific fields we want to analyze. As we do this, however, we should keep in mind that all of the original raw data still exists, should we ever need it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetch LPP Tickets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminding ourselves of our question:\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "So, let's start by making sure that we can some time-related metadata from an LPP ticket. We'll look at LPP tickets that relate to DXP, but are not any of the known workflow testing tickets (because those aren't really DXP tickets, even if they have a DXP affected version)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lpp_jql = \"\"\"\n",
    "project = LPP and affectedVersion = \"7.0 DE (7.0.10)\" and\n",
    "key not in (LPP-10825,LPP-10826,LPP-12114,LPP-13367)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIRA allows you to perform a join as part of its API by requesting an expansion of certain fields. Because we're interested in the time the ticket spends in each status, we can ask it to expand the `changelog` field, which effectively asks JIRA to perform a join on its equivalent of a `changelog` table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    lpp_issues = get_jira_issues(lpp_jql, ['changelog'])\n",
    "else:\n",
    "    lpp_issues = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lpp_issues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Resolution Transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the raw data:\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "To answer that question, we'll first need to accumulate all of the transitions. One way to think about active tickets is to look at all tickets that are unresolved on any given day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile unresolved.py\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "import dateparser\n",
    "from six import string_types\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "def extract_time(issue):\n",
    "    if region_field_name in issue['fields'] and issue['fields'][region_field_name] is not None:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "    else:\n",
    "        regions = ['']\n",
    "\n",
    "    old_resolution = None\n",
    "    old_status_date = dateparser.parse(issue['fields']['created'])\n",
    "\n",
    "    history_entries = issue['changelog']['histories']\n",
    "\n",
    "    for history_entry in history_entries:\n",
    "        history_entry['createdTime'] = dateparser.parse(history_entry['created'])\n",
    "\n",
    "    transitions = []\n",
    "\n",
    "    for history_entry in history_entries:\n",
    "        useful_history_items = [\n",
    "            item for item in history_entry['items']\n",
    "                if item['field'] == 'resolution'\n",
    "        ]\n",
    "\n",
    "        if len(useful_history_items) == 0:\n",
    "            continue\n",
    "\n",
    "        new_status_date = history_entry['createdTime']\n",
    "\n",
    "        for item in useful_history_items:\n",
    "            new_resolution = item['toString']\n",
    "\n",
    "            if old_resolution is None and new_resolution is not None:\n",
    "                transitions.append({\n",
    "                    'jiraKey': issue['key'],\n",
    "                    'type': issue['fields']['issuetype']['name'],\n",
    "                    'region': regions[0],\n",
    "                    'statusStart': old_status_date.date(),\n",
    "                    'statusEnd': new_status_date.date()\n",
    "                })\n",
    "\n",
    "            old_resolution = new_resolution\n",
    "\n",
    "        if old_resolution is None:\n",
    "            old_status_date = new_status_date\n",
    "\n",
    "    if old_resolution is None:\n",
    "        transitions.append({\n",
    "            'jiraKey': issue['key'],\n",
    "            'type': issue['fields']['issuetype']['name'],\n",
    "            'region': regions[0],\n",
    "            'statusStart': old_status_date.date(),\n",
    "            'statusEnd': date.today()\n",
    "        })\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block uses the `reload` ability to allow us to constantly change the time extraction above (which writes to a separate Python file so that it can be parallelized) and then reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unresolved\n",
    "reload(unresolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we perform our parallel processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "num_finished = 0\n",
    "\n",
    "for result in pool.imap_unordered(unresolved.extract_time, lpp_issues.values()):\n",
    "    if num_finished % 100 == 0:\n",
    "        print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))\n",
    "\n",
    "    num_finished += 1\n",
    "\n",
    "    for entry in result:\n",
    "        times.append(entry)\n",
    "\n",
    "print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Cross-Tabulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin creating tables that summarize the table across a numerical statistic (in our case, counts), it's useful to do some data validation so that we can check our assumptions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One validation we might do is this: how much tickets of each type are open in each region? This question can be answered with a contingency table, or a cross-tabulation, which is available in almost all statistic computing libraries.\n",
    "\n",
    "* [Crosstabs](http://libguides.library.kent.edu/SPSS/Crosstabs)\n",
    "\n",
    "For those who are familiar with Excel pivot table, you can think of a cross-tabulations as the simplest pivot table, where each data point has a value of 1 and you're only computing the sum.\n",
    "\n",
    "* [Pivot table](https://exceljet.net/things-to-know-about-excel-pivot-tables)\n",
    "\n",
    "In our case, the function we're using to compute the value of each cell is to simply count them. The result looks at how two or more columns co-occur with each other, and it's especially useful when examining how two categorical variables relate to each other.\n",
    "\n",
    "* [Categorical Variable](https://en.wikipedia.org/wiki/Categorical_variable)\n",
    "\n",
    "This allows you to check any assumptions about the data, such as whether two column values never co-occur, or whether two column values seem to co-occur a lot more than expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Discrete Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might wonder, can you cross-tabulate two non-categorical columns, such as two columns that are both continuous, floating point values?\n",
    "\n",
    "* [Continuous and Discrete Variable](https://en.wikipedia.org/wiki/Continuous_and_discrete_variable)\n",
    "\n",
    "If you think about it, a cross-tabulation will have too many columns and too many rows. In this case, if you are prefer to use a table to start out, one popular option is to simply convert the continuous variable into a discrete variable using binning, which allows you to tabulate the resulting artificial categories. This is something that is common within pivot tables as well.\n",
    "\n",
    "* [Data binning](https://en.wikipedia.org/wiki/Data_binning)\n",
    "\n",
    "For the more mathematics and visually oriented, this table might over-summarize the information if you do not choose your discretization (or binning) function well. An alternative popular option is to simply interpret the co-occurrences as probabilities rather than raw counts, which allows you to construct a contour plot of the the joint probability function.\n",
    "\n",
    "If you've never heard of a contour plot before, imagine that one variable is geo latitude, and another variable is geo longitude. A famous contour plot would be something that depicts the elevation at each geo latitude. The end result is an elevation map, which you can render in two dimensions with color, or allow interacting in three dimensions like in Google Earth.\n",
    "\n",
    "* [Contour plots](http://www.statisticshowto.com/contour-plots/)\n",
    "\n",
    "If you don't remember what a joint probability function is, and you're a visual learner, these lecture notes give a good visual refresher, and also provide visualizations that help you connect the idea behind a joint probability distribution to contour plots.\n",
    "\n",
    "* [Joint Density Functions, Marginal Density Functions, Conditional Density Functions, Expectations and Independence](http://www.colorado.edu/economics/morey/7818/jointdensity/jointdensity.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our filtered view of the JIRA data, let's revisit our research question.\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "Given our question, naturally, our next step is to visualize our JIRA data. We'll start with the most basic visualization: a table.\n",
    "\n",
    "In this case, we'll take a look at the cross-tabulation of ticket types and regions. Note that because our table allows for a ticket to appear multiple times (because tickets can become resolved and then be reopened), this table is interpreted as how often 7.0.x tickets of different types have been flagged as resolved in each region, according to the JIRA changelog entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['region'], [df['type']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the above that there are some activity transitions where the tickets have no region at all. DXP Product Escalations have tickets that appear to have a region field, while Sub-Task tickets exclusively have no region.\n",
    "\n",
    "If we were to do some data cleansing, we might go revisit those in order to fill in the appropriate region in case it is relevant to other data analysis tasks. As we are summarizing the data using visualizations, these will only matter if we were to create region-specific visualizations of these activity counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(HTML(', '.join([\n",
    "    '<nobr><a href=\"http://liferay.atlassian.net/browse/%s\">%s</a></nobr>' % (jira_key, jira_key)\n",
    "        for jira_key in df[df['region'] == '']['jiraKey'].unique()\n",
    "])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Time Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we've actually achieved is one of the most common groupings: time.\n",
    "\n",
    "When you group data over time and then visualize it, essentially you are monitoring is what is happening to a specific value, with data spaced out at equal intervals (hours, days, weeks, months, years), which opens up a lot of different ways to both analyze and transform the time data. The types of transformations and analyses you can readily do depend on the data set and your background in applied mathematics.\n",
    "\n",
    "* [Time series](https://en.wikipedia.org/wiki/Time_series)\n",
    "\n",
    "For the purposes of this notebook, we aren't going to try to analyze the data. Rather, we're going to try to apply a simple time-based summarization: the total number of active tickets, with varying definitions of active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Load: Raw Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's come back to our research question.\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "One of the most natural is to simply ask, \"How many tickets were active today?\" Then ask that question every day.\n",
    "\n",
    "So let's say that you wanted to compute how many tickets were active on each day given our current table, which has a column that indicates when a ticket started being active and a column that indicates when a ticket was no longer active. How can you use this table to compute how many tickets were active on any given day?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def expand_as_days(start, end):\n",
    "    current = start\n",
    "\n",
    "    while current <= end:\n",
    "        yield current\n",
    "        current += timedelta(days=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A naive approach is to simply generate a new mapping table similar to the tables we've already created, where we create an entry for every single day a ticket is active. Since we don't actually have that many tickets and that many days, this is actually extremely practical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_active_mapping(df, expand_function):\n",
    "    active_by_date = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "    for jira_key, region, start, end in zip(df['jiraKey'], df['region'], df['statusStart'], df['statusEnd']):\n",
    "        for current in expand_function(start, end):\n",
    "            active_by_date[region][current].add(jira_key)\n",
    "\n",
    "    df_active_by_day_mapping = pd.DataFrame([\n",
    "        {'date': date_key, 'region': region, 'count': len(jira_keys)}\n",
    "            for region, value in active_by_date.items()\n",
    "                for date_key, jira_keys in sorted(value.items())\n",
    "    ])\n",
    "\n",
    "    return df_active_by_day_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that information, we can achieve what we want through a group by expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_active = get_active_mapping(df, expand_as_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df_active[['date', 'count']].groupby(['date'])\n",
    "\n",
    "df_count = df_groupby.sum()\n",
    "df_count.columns = ['Global Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see from this visualization is that the tickets are sticking around. This suggests that some tickets never enter a resolved state, and these are causing our visualization to show that tickets are persisting.\n",
    "\n",
    "So this raises a question. Of the tickets that are supposedly not yet resolved today, what's their current resolution status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_today = df[df['statusEnd'] == date.today()].copy()\n",
    "df_today['statusStartMonth'] = df_today['statusStart'].apply(lambda x: x.replace(day=1))\n",
    "\n",
    "all_resolutions = [\n",
    "    {\n",
    "        'jiraKey': issue['key'],\n",
    "        'resolution': issue['fields']['resolution']['name']\n",
    "            if issue['fields']['resolution'] is not None else 'Unresolved'\n",
    "    }\n",
    "    for issue in lpp_issues.values()\n",
    "]\n",
    "\n",
    "df_resolutions = pd.DataFrame(all_resolutions)\n",
    "\n",
    "df_today_resolutions = df_today.merge(df_resolutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us to the following distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df_today_resolutions['resolution']).plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualization lets us know that while most of the tickets that lack a resolution history entry are truly unresolved, some of these tickets actually lack a history entry, because they aren't unresolved. This is essentially case of dirty data.\n",
    "\n",
    "* [Dirty data](https://en.wikipedia.org/wiki/Dirty_data)\n",
    "\n",
    "There are a variety of ways to handle dirty data, such as imputing all the missing values. For now, we'll look at a different way of extracting whether or not a ticket is active."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Status Transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll look at status transitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile startstop.py\n",
    "from collections import defaultdict\n",
    "from datetime import date\n",
    "import dateparser\n",
    "from six import string_types\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "def extract_time(issue):\n",
    "    if region_field_name in issue['fields'] and issue['fields'][region_field_name] is not None:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "    else:\n",
    "        regions = ['']\n",
    "\n",
    "    old_status = 'Open'\n",
    "    old_status_date = dateparser.parse(issue['fields']['created'])\n",
    "\n",
    "    history_entries = issue['changelog']['histories']\n",
    "\n",
    "    for history_entry in history_entries:\n",
    "        history_entry['createdTime'] = dateparser.parse(history_entry['created'])\n",
    "\n",
    "    transitions = []\n",
    "\n",
    "    for history_entry in history_entries:\n",
    "        useful_history_items = [\n",
    "            item for item in history_entry['items']\n",
    "                if item['field'] == 'status'\n",
    "        ]\n",
    "\n",
    "        if len(useful_history_items) == 0:\n",
    "            continue\n",
    "\n",
    "        new_status_date = history_entry['createdTime']\n",
    "\n",
    "        for item in useful_history_items:\n",
    "            new_status = item['toString']\n",
    "\n",
    "            transitions.append({\n",
    "                'jiraKey': issue['key'],\n",
    "                'type': issue['fields']['issuetype']['name'],\n",
    "                'region': regions[0],\n",
    "                'status': old_status,\n",
    "                'statusStart': old_status_date.date(),\n",
    "                'statusEnd': new_status_date.date()\n",
    "            })\n",
    "\n",
    "            old_status = new_status\n",
    "\n",
    "    transitions.append({\n",
    "        'jiraKey': issue['key'],\n",
    "        'type': issue['fields']['issuetype']['name'],\n",
    "        'region': regions[0],\n",
    "        'status': old_status,\n",
    "        'statusStart': old_status_date.date(),\n",
    "        'statusEnd': date.today()\n",
    "    })\n",
    "\n",
    "    return transitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block uses the `reload` ability to allow us to constantly change the time extraction above (which writes to a separate Python file so that it can be parallelized) and then reload it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import startstop\n",
    "reload(startstop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we process all of our issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = []\n",
    "\n",
    "region_field_name = 'customfield_11523'\n",
    "\n",
    "num_finished = 0\n",
    "\n",
    "for result in pool.imap_unordered(startstop.extract_time, lpp_issues.values()):\n",
    "    if num_finished % 100 == 0:\n",
    "        print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))\n",
    "\n",
    "    num_finished += 1\n",
    "\n",
    "    for entry in result:\n",
    "        times.append(entry)\n",
    "\n",
    "print('[%s] Processed %d of %d issues' % (datetime.now().isoformat(), num_finished, len(lpp_issues)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Active Statuses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've extracted all of the status transitions:\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "First, we'll need to identify which statuses correspond to a ticket being active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['status'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we see that list, almost all the statuses could be considered active other than Audit and Closed. The remaining statuses depend on whether you're looking at things from the Customer Support perspective or the Technical Support perspective.\n",
    "\n",
    "We can capture whether or not a ticket is active from the Customer Support perspective through LESA. Therefore, it makes sense to focus on the Technical Support active statuses. These are the statuses we'll treat as active:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "active_statuses = set([\n",
    "    'Open', 'Verified', 'In Progress', 'On Hold', 'In Review',\n",
    "    'Ready for Investigation', 'Reopened', 'Wormhole',\n",
    "    'Awaiting Help', 'Developing Plan'\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using that, we filter our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = df[df['status'].isin(active_statuses)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have that information, we can achieve what we want through a group by expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_active = get_active_mapping(df, expand_as_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_groupby = df_active[['date', 'count']].groupby(['date'])\n",
    "\n",
    "df_count = df_groupby.sum()\n",
    "df_count.columns = ['Global Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_count.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Specific Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the things we have to worry about when plotting a region-specific visualization is the number of regions we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [region for region in sorted(df_active['region'].unique()) if region != '']\n",
    "\n",
    "regions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand why, let's go ahead and plot all of our regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in regions:\n",
    "    df_region = df_active[df_active['region'] == region][['date', 'count']]\n",
    "    plt.plot_date(df_region['date'], df_region['count'], fmt='-', label='%s Count' % region)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we have seven regions and only six unique colors in our color palette, and so the last color repeats. Luckily, the regions have very different ticket counts for ee-7.0.x so it's not a problem per se, but it's a useful talking point.\n",
    "\n",
    "When you run out into this situation, there are many options available to you. Some of the most popular are to choose what you wish to see (so don't plot all the regions), add more colors (which means you have to know for sure what kinds of color blindness exist in your target audience), or group together the different items and create multiple plots.\n",
    "\n",
    "In this case, it looks like if we were to group in a typical negative UTC offset (US, Brazil) and positive UTC offset (Japan, EU, Spain, APAC, India), we have two graphs that have two and five colors, which is just enough of a division for us to distinguish colors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_utc_offset = ['Brazil', 'US']\n",
    "positive_utc_offset = ['APAC', 'EU', 'India', 'Japan', 'Spain']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for region in negative_utc_offset:\n",
    "    df_region = df_active[df_active['region'] == region][['date', 'count']]\n",
    "    plt.plot_date(df_region['date'], df_region['count'], fmt='-', label='%s Count' % region)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "for region in positive_utc_offset:\n",
    "    df_region = df_active[df_active['region'] == region][['date', 'count']]\n",
    "    plt.plot_date(df_region['date'], df_region['count'], fmt='-', label='%s Count' % region)\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Load: Change Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "You can think of the raw counts as having accumulated from tickets that became active and tickets that went inactive. Visualizing these increases and decreases along with the net change is also another way to visualize our counts.\n",
    "\n",
    "First, let's compute these increases and decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ActiveCount = namedtuple('ActivityCount', ['date', 'region', 'added', 'removed', 'net_change'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_active_changes(df, expand_function):\n",
    "    active_by_date = defaultdict(lambda: defaultdict(set))\n",
    "\n",
    "    for jira_key, region, start, end in zip(df['jiraKey'], df['region'], df['statusStart'], df['statusEnd']):\n",
    "        for current in expand_function(start, end):\n",
    "            active_by_date[region][current].add(jira_key)\n",
    "\n",
    "    changes = []\n",
    "\n",
    "    for region, values in active_by_date.items():\n",
    "        old_active = set()\n",
    "\n",
    "        for date, new_active in sorted(values.items()):\n",
    "            added = len(new_active - old_active)\n",
    "            removed = 0 - len(old_active - new_active)\n",
    "            net_change = len(new_active) - len(old_active)\n",
    "\n",
    "            changes.append(ActiveCount(date, region, added, removed, net_change))\n",
    "\n",
    "            old_active = new_active\n",
    "\n",
    "    return pd.DataFrame(changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_changes = get_active_changes(df, expand_as_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_groupby = df_changes[['date', 'added', 'removed', 'net_change']].groupby('date')\n",
    "\n",
    "df_total_changes = df_groupby.sum()\n",
    "\n",
    "plt.plot_date(df_total_changes.index, df_total_changes['added'], fmt='^')\n",
    "plt.plot_date(df_total_changes.index, df_total_changes['removed'], fmt='v')\n",
    "plt.plot_date(df_total_changes.index, df_total_changes['net_change'], fmt='--')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Visualizing Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as you can visualize values over time, you can also ignore the time dimension and simply visualize the values themselves.\n",
    "\n",
    "* [Visualizing Distributions](http://www.darkhorseanalytics.com/blog/visualizing-distributions-3)\n",
    "\n",
    "The goal of these visualizations is to understand where values are concentrated, and they give you a visual sense of whether this looks like a sequence of numbers that the world has seen and studied before (because a lot of analysis is comparing what you see to what you already understand).\n",
    "\n",
    "* [Common Probability Distributions: A Data Scientist's Crib Sheet](https://blog.cloudera.com/blog/2015/12/common-probability-distributions-the-data-scientists-crib-sheet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Load: Central Tendency, Take 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's come back to our research question.\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "As we just reviewed, another way you can visualize ticket activity is to compare today's ticket counts with all ticket counts independent of time. We'll do this by combining two visualizations: a histogram and a box and whiskers plot.\n",
    "\n",
    "* [Histogram](https://en.wikipedia.org/wiki/Histogram)\n",
    "* [Box plot](https://en.wikipedia.org/wiki/Box_plot)\n",
    "\n",
    "When we're plotting this data, we may want to worry about weekends, because we usually don't see much LPP ticket activity over the weekends, and so it will cause whatever ticket counts occurred on Friday will tend to repeat. At the same time, it might not make any difference in the shape of the distribution (just the raw values will be lower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_weekday = df_active['date'].apply(lambda x: x.weekday() < 5)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, nrows=2, sharex=True, gridspec_kw={\"height_ratios\": (.05, .95)})\n",
    "\n",
    "count = df_active[['date', 'count']].groupby(['date']).sum()['count']\n",
    "\n",
    "sns.boxplot(count, ax=axes[0][0])\n",
    "sns.distplot(count, ax=axes[1][0], kde=False, rug=True)\n",
    "\n",
    "axes[0][0].set_xlabel('')\n",
    "axes[1][0].set_title('Include Weekends')\n",
    "axes[1][0].set_xlabel('Global Count')\n",
    "\n",
    "count = df_active[is_weekday][['date', 'count']].groupby(['date']).sum()['count']\n",
    "\n",
    "sns.boxplot(count, ax=axes[0][1])\n",
    "sns.distplot(count, ax=axes[1][1], kde=False, rug=True)\n",
    "\n",
    "axes[0][1].set_xlabel('')\n",
    "axes[1][1].set_title('Exclude Weekends')\n",
    "axes[1][1].set_xlabel('Global Count')\n",
    "\n",
    "axes[1][1].set_ylim(axes[1][0].get_ylim())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the histogram, we get a sense of the shape of the data. The x-axis represents the number of tickets that were active, where each block represents a range of values (from 0 to 19, for example), and the y-axis represents the number of times we saw that range of active ticket counts in our data set. In this case, that means the number of days we saw those values.\n",
    "\n",
    "In the box and whiskers plot, we get a sense of the concentration of the data. The width of the shaded box represents the range of values between the first and third quartiles, the heavy black line in the middle is the second quartile (also known as the median), and the whiskers are intended to be 1.5 times the difference between the first and third quartiles, bounded by the minimum and maximum values in the actual data.\n",
    "\n",
    "* [Quartile](https://en.wikipedia.org/wiki/Quartile)\n",
    "* [Interquartile Range](https://en.wikipedia.org/wiki/Interquartile_range)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Load: Central Tendency, Take 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we ask our question.\n",
    "\n",
    "<b style=\"color:green\">How can we visualize LPP ticket activity for DXP?</b>\n",
    "\n",
    "Just as when we looked at change counts over time, we can also look at change counts independent of time, and we can see how these changes are concentrated.\n",
    "\n",
    "In this case, understanding the counts for a specific region is more useful than seeing the counts globally. Below, we choose a region for our visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_region = 'US'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_region_changes = df_changes[df_changes['region'] == selected_region]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When looking at the change counts, we have the same problem as when visualizing the total counts, where there is less activity on weekends. In this case, rather than repetition, we expect that it will increase the number of small values (especially zero), due to the low activity over the weekends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_changes_plot(column, title):\n",
    "    is_weekday = df_region_changes['date'].apply(lambda x: x.weekday() < 5)\n",
    "\n",
    "    fig, axes = plt.subplots(ncols=2, nrows=2, sharex=True, gridspec_kw={\"height_ratios\": (.05, .95)})\n",
    "\n",
    "    count = df_region_changes[['date', column]].groupby('date').sum()[column]\n",
    "\n",
    "    sns.boxplot(count, ax=axes[0][0])\n",
    "    sns.distplot(count, ax=axes[1][0], bins=10, kde=False, rug=False)\n",
    "\n",
    "    axes[0][0].set_xlabel('')\n",
    "    axes[1][0].set_title('Include Weekends')\n",
    "    axes[1][0].set_xlabel(title)\n",
    "\n",
    "    count = df_region_changes[is_weekday][['date', column]].groupby('date').sum()[column]\n",
    "\n",
    "    sns.boxplot(count, ax=axes[0][1])\n",
    "    sns.distplot(count, ax=axes[1][1], bins=10, kde=False, rug=False)\n",
    "\n",
    "    axes[0][1].set_xlabel('')\n",
    "    axes[1][1].set_title('Exclude Weekends')\n",
    "    axes[1][1].set_xlabel(title)\n",
    "\n",
    "    axes[1][1].set_ylim(axes[1][0].get_ylim())\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_changes_plot('added', 'Newly Active')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_changes_plot('removed', 'Newly Inactive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_changes_plot('net_change', 'Net Change')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Load: Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we've visualized central tendency, and we've visualized the distribution of the different values, the next thing on our plate is to visualize uncertainty. Uncertainty is rooted in the idea of a prediction, and uncertainty is how wrong you believe that prediction might be.\n",
    "\n",
    "For the purpose of explaining uncertainty, let's take a look at the net change in tickets for our selected region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot_date(df_region_changes['date'], df_region_changes['net_change'], fmt='.')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of uncertainty is to first make a prediction about what we think the value should be. Since this is a time series, there's a lot of different analysis you do as you're choosing a model, but we'll do something extremely simple to start: we guess the value as the average value for the previous 30 days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_window = df_region_changes['net_change'].rolling(30, 1)\n",
    "\n",
    "guesses = rolling_window.mean()\n",
    "shifted_guesses = np.concatenate([[0], guesses[:-1]])\n",
    "\n",
    "plt.plot_date(df_region_changes['date'], df_region_changes['net_change'], fmt='.')\n",
    "plt.plot_date(df_region_changes['date'], shifted_guesses, '-')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, our guess is somewhat educated, but as you can see from the plot above, we are actually usually way off from the guess. Uncertainty is the idea of just how confident you are in your guess and is represented as a confidence or prediction interval.\n",
    "\n",
    "* [Confidence and Prediction Bands](https://en.wikipedia.org/wiki/Confidence_and_prediction_bands)\n",
    "\n",
    "If you've chosen the average as your measure of central tendency, then if you make certain assumptions about your data (which may or may not be true), you might choose the standard deviation as a measure of uncertainty.\n",
    "\n",
    "* [Standard deviation](https://en.wikipedia.org/wiki/Standard_deviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guesses_std = rolling_window.std()\n",
    "\n",
    "shifted_guesses_std = np.concatenate([[0, 0], guesses_std[1:-1]])\n",
    "\n",
    "shifted_guesses_std\n",
    "\n",
    "plt.plot_date(df_region_changes['date'], df_region_changes['net_change'], fmt='.')\n",
    "plt.plot_date(df_region_changes['date'], shifted_guesses, '-')\n",
    "\n",
    "plt.fill_between(\n",
    "    df_region_changes['date'].values,\n",
    "    shifted_guesses - 2*shifted_guesses_std,\n",
    "    shifted_guesses + 2*shifted_guesses_std,\n",
    "    alpha=0.2,\n",
    "    color=sns.color_palette()[0]\n",
    ")\n",
    "\n",
    "plt.fill_between(\n",
    "    df_region_changes['date'].values,\n",
    "    shifted_guesses - shifted_guesses_std,\n",
    "    shifted_guesses + shifted_guesses_std,\n",
    "    alpha=0.2,\n",
    "    color=sns.color_palette()[1]\n",
    ")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above, the shaded regions represents definitions of uncertainty, if we were to use one standard deviation as our measure of uncertainty, the light green region represents our uncertainty around that estimate. If we were to use two standard deviations as our measure of uncertainty, the light blue region represents that uncertainty."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
