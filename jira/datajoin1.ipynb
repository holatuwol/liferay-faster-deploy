{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database Joins 1: Combining JIRA With GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, our research question is as follows:\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "In order to answer this question, this notebook introduces database joins. However, rather than teaching you what a database join looks like in SQL, it instead shows you a very simple database join in code while presenting some of the underlying implementation concepts.\n",
    "\n",
    "At the end of this notebook, we will have a script that combines data retrieved from JIRA with data retrieved from GitHub, and the reader will have an improved understanding of what goes into a join."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell attempts to use `conda` and `pip` to install the libraries that are used by this notebook. If the output indicates that additional items were installed, you will need to restart the kernel after the installation completes before you can run the later cells in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -y numpy pandas pytz requests ujson\n",
    "!pip install dateparser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from collections import defaultdict, namedtuple\n",
    "import dateparser\n",
    "from datetime import date, datetime\n",
    "import hashlib\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import re\n",
    "import requests\n",
    "import six\n",
    "import sys\n",
    "import subprocess\n",
    "import ujson as json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start, we'll make sure that we establish one rule for this script and all future scripts: we will save all raw data with timestamps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "today = date.today()\n",
    "now = datetime.now(pytz.utc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important, because one of the more common things to do as a developer is to retrieve the data, extract only the information you need, and then discard the data you do not need. However, unless you have some terms of service agreement restricting what data you are allowed to retain, do not discard the raw data! Raw data is a starting point that speeds up the creation of many different application prototypes.\n",
    "\n",
    "We could serialize all the data as a single JSON object. In this case, however, we're going to adopt a slightly less conventional data format, as it will make the future tutorials talking about database indices much easier to explain. The file format is as follows for each key-value pair:\n",
    "\n",
    "```\n",
    "primkey [TAB] index_field_1 [TAB] index_field_2 [TAB] ... [TAB] index_field_n [TAB] row_value\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_file_name(cache_name, suffix):\n",
    "    base_name = os.path.basename(cache_name)\n",
    "    subfolder_name = os.path.dirname(cache_name)\n",
    "    folder_name = 'rawdata/%s' % subfolder_name\n",
    "\n",
    "    if not os.path.exists(folder_name):\n",
    "        os.makedirs(folder_name)\n",
    "\n",
    "    return '%s/%s_%s.%s' % (folder_name, today.isoformat(), base_name, suffix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def save_row(outfile, keys, row_value):\n",
    "    for key in keys:\n",
    "        outfile.write(json.dumps(key))\n",
    "        outfile.write('\\t')\n",
    "\n",
    "    outfile.write(json.dumps(row_value))\n",
    "    outfile.write('\\n')\n",
    "    \n",
    "def save_raw_dict(cache_name, raw_dict, index_fields=[]):\n",
    "    file_name = get_file_name(cache_name, '.json')\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        for primkey, row_value in raw_dict.items():\n",
    "            keys = [primkey] + [row[field] if field in row else None for field in index_fields]\n",
    "            save_row(outfile, keys, row_value)\n",
    "    \n",
    "    return load_raw_dict(cache_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_row(line):\n",
    "    row = line.split('\\t')\n",
    "    keys = [json.loads(key) for key in row[0:-1]]\n",
    "    return keys, json.loads(row[-1])\n",
    "\n",
    "def load_raw_dict(cache_name):\n",
    "    file_name = get_file_name(cache_name, '.json')\n",
    "\n",
    "    if not os.path.isfile(file_name):\n",
    "        return None\n",
    "\n",
    "    raw_dict = {}\n",
    "    indexed_data = defaultdict(list)\n",
    "    \n",
    "    with open(file_name) as infile:\n",
    "        for line in infile:\n",
    "            keys, row_value = load_row(line)\n",
    "\n",
    "            primkey = keys[0]\n",
    "            raw_dict[primkey] = row_value\n",
    "\n",
    "            index_fields = tuple(keys[1:])\n",
    "            \n",
    "            if len(index_fields) > 0:\n",
    "                indexed_data[index_fields].append(row_value)\n",
    "\n",
    "    if len(indexed_data) == 0:\n",
    "        return raw_dict\n",
    "            \n",
    "    return raw_dict, indexed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from JIRA, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's keep our question in mind during each step.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Before we can answer the question of whether anything is *stuck in review*, we will need to answer the broader question of what is *in review*. We'll start by looking at what's in review in JIRA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to JIRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because [issues.liferay.com](https://issues.liferay.com/) does not have OAuth support, we will need to find a different way to connect to our JIRA installation. The simplest way is to simply login to JIRA.\n",
    "\n",
    "There are a lot of secure ways to specify your username and password, but for the sake of this script, we'll use the most insecure way possible: a plain text file. Namely, the `.gitconfig` in your user's home folder. If you want to use a different strategy that's less global (a plain text JSON file in the same folder as this script, for example) or more secure, just change the implementation of the two functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_config(key):\n",
    "    try:\n",
    "        return subprocess.check_output(['git', 'config', key]).strip().decode('utf8')\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def set_config(key, value):\n",
    "    subprocess.call(['git', 'config', '--global', key, value])\n",
    "    subprocess.call(['git', 'config', '--global', key, value])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming you are using the default implementation, set your username and password inside of the `.gitconfig` located in your user's home folder by running the following two commands in a command line window.\n",
    "\n",
    "```\n",
    "git config --global jira.session-username JIRA_USERNAME\n",
    "git config --global jira.session-password JIRA_PASSWORD\n",
    "```\n",
    "\n",
    "The following cell will read it in and confirm that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jira_username = get_config('jira.session-username')\n",
    "jira_password = get_config('jira.session-password')\n",
    "\n",
    "assert(jira_username is not None)\n",
    "assert(jira_password is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use this information and attempt to login to JIRA and confirm that it has a valid session, which will confirm that the credentials you saved are valid. It will also save this session information so that it can reuse it later without constantly relogging in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jira_base_url = 'https://issues.liferay.com/rest'\n",
    "\n",
    "def get_jira_cookie():\n",
    "    jira_cookie = None\n",
    "\n",
    "    jira_cookie_name = None\n",
    "    jira_cookie_value = None\n",
    "\n",
    "    try:\n",
    "        jira_cookie_name = get_config('jira.session-cookie-name')\n",
    "        jira_cookie_value = get_config('jira.session-cookie-value')\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if jira_cookie_name is not None and jira_cookie_value is not None:\n",
    "        jira_cookie = {\n",
    "            jira_cookie_name: jira_cookie_value\n",
    "        }\n",
    "\n",
    "        r = requests.get(jira_base_url + '/auth/1/session', cookies=jira_cookie)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            jira_cookie = None\n",
    "\n",
    "    if jira_cookie is not None:\n",
    "        return jira_cookie\n",
    "        \n",
    "    post_json = {\n",
    "        'username': jira_username,\n",
    "        'password': jira_password\n",
    "    }\n",
    "\n",
    "    r = requests.post(jira_base_url + '/auth/1/session', json=post_json)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        print('Invalid login')\n",
    "\n",
    "        return None\n",
    "\n",
    "    response_json = r.json()\n",
    "\n",
    "    jira_cookie_name = response_json['session']['name']\n",
    "    jira_cookie_value = response_json['session']['value']\n",
    "\n",
    "    set_config('jira.session-cookie-name', jira_cookie_name)\n",
    "    set_config('jira.session-cookie-value', jira_cookie_value)\n",
    "\n",
    "    jira_cookie = {\n",
    "        jira_cookie_name: jira_cookie_value\n",
    "    }\n",
    "\n",
    "    return jira_cookie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assert(get_jira_cookie() is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a valid login, our next step is to use the JIRA API to retrieve tickets. If you've interacted with JIRA before, you know that it has its own query language (JQL). It turns out there is a simple search API that allows you to submit the JQL and all the matching issues are returned as JSON. Since the API is fairly simple, we implement it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_jira_issues(jql):\n",
    "    jira_cookie = get_jira_cookie()\n",
    "\n",
    "    if jira_cookie is None:\n",
    "        return []\n",
    "\n",
    "    start_at = 0\n",
    "\n",
    "    post_json = {\n",
    "        'jql': jql,\n",
    "        'startAt': start_at\n",
    "    }\n",
    "\n",
    "    r = requests.post(jira_base_url + '/api/2/search', cookies=jira_cookie, json=post_json)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return {}\n",
    "\n",
    "    response_json = r.json()\n",
    "\n",
    "    issues = {}\n",
    "    \n",
    "    for issue in response_json['issues']:\n",
    "        issues[issue['key']] = issue\n",
    "\n",
    "    while start_at + response_json['maxResults'] < response_json['total']:\n",
    "        start_at += response_json['maxResults']\n",
    "        post_json['startAt'] = start_at\n",
    "\n",
    "        r = requests.post(jira_base_url + '/api/2/search', cookies=jira_cookie, json=post_json)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            return issues\n",
    "\n",
    "        response_json = r.json()\n",
    "\n",
    "        for issue in response_json['issues']:\n",
    "            issues[issue['key']] = issue\n",
    "\n",
    "    return issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have something that can retrieve JIRA issues, all we need is to actually create our JQL and then run the search. This is the JQL we'll use for regular Liferay Portal Patch (LPP) issues that are in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "in_review_jql = '''\n",
    "    project = LPP AND\n",
    "    type not in (\"SME Request\", \"SME Request SubTask\") AND\n",
    "    status = \"In Review\"\n",
    "    order by key\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and retrieve those results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jql_hashes = load_raw_dict('jql_hashes')\n",
    "\n",
    "if jql_hashes is None:\n",
    "    jql_hashes = {}\n",
    "\n",
    "def get_jql_hashed_name(base_name, jql):\n",
    "    jql_hash = None\n",
    "    \n",
    "    for key, value in jql_hashes.items():\n",
    "        if value == jql:\n",
    "            jql_hash = key\n",
    "            break\n",
    "\n",
    "    if jql_hash is None:            \n",
    "        digester = hashlib.md5()\n",
    "        digester.update(jql)\n",
    "        jql_hash = digester.hexdigest()\n",
    "\n",
    "        jql_hashes[jql_hash] = jql\n",
    "\n",
    "        save_raw_dict('jql_hashes', jql_hashes)\n",
    "    \n",
    "    return '%s/%s' % (jql_hash, base_name)\n",
    "\n",
    "def get_jira_issues(jql):\n",
    "    base_name = get_jql_hashed_name('jira_issues', jql)\n",
    "\n",
    "    jira_issues = load_raw_dict(base_name)\n",
    "\n",
    "    if jira_issues is not None:\n",
    "        print('Loaded cached JIRA search')\n",
    "        return jira_issues\n",
    "\n",
    "    print('Executing JIRA search')\n",
    "\n",
    "    jira_issues = retrieve_jira_issues(in_review_jql)\n",
    "    jira_issues = save_raw_dict(base_name, jira_issues)\n",
    "    return jira_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    jira_issues = get_jira_issues(in_review_jql)\n",
    "else:\n",
    "    jira_issues = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at JSON is a bit tedious, so we'll take a look at a subset of fields in a way that resembles the view you get when you run JQL via the web browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JIRAIssue = namedtuple(\n",
    "    'JIRAIssue',\n",
    "    ['ticket_key', 'region', 'status', 'assignee', 'summary']\n",
    ")\n",
    "\n",
    "def get_jira_tuple(issue):\n",
    "    region_field_name = 'customfield_11523'\n",
    "\n",
    "    regions = ['']\n",
    "\n",
    "    if region_field_name in issue['fields']:\n",
    "        regions = [region['value'] for region in issue['fields'][region_field_name]]\n",
    "\n",
    "    return JIRAIssue(\n",
    "        ticket_key=issue['key'],\n",
    "        region=regions[0],\n",
    "        status=issue['fields']['status']['name'],\n",
    "        assignee=issue['fields']['assignee']['displayName'],\n",
    "        summary=issue['fields']['summary']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_jira_tuple(issue) for issue in jira_issues.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub, Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's remind ourselves of our original question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "What is this concept of \"in review\"? Well, being \"in review\" actually means that your code has been written and you are now waiting on another team member to look at your changes, providing a sanity check from someone who is looking at the solution with fresh eyes.\n",
    "\n",
    "Where do these sanity check reviews occur? They occur on GitHub. This means that we will want to bring in the GitHub data set in order to answer our question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Login to GitHub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luckily, [api.github.com](https://developer.github.com/v3/) does have OAuth support, so we'll want to request an OAuth token from GitHub and leverage it in our script.\n",
    "\n",
    "* [Creating a personal access token for command line](https://help.github.com/articles/creating-a-personal-access-token-for-the-command-line/)\n",
    "\n",
    "Assuming you are using the default implementation for configuration values provided by this notebook (mentioned earlier when setting up JIRA access), set your OAuth token inside of the `.gitconfig` located in your user's home folder by running the following command in a command line window.\n",
    "\n",
    "```\n",
    "git config --global github.oauth-token GITHUB_OAUTH_TOKEN\n",
    "```\n",
    "\n",
    "If you customized it, do whatever you need to get the configuration value persisted. The following cell will read it in and confirm that it exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "github_oauth_token = get_config('github.oauth-token')\n",
    "\n",
    "assert(github_oauth_token is not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it's not enough that it exists. We should confirm that the token works on the repositories that are related to Liferay. We'll use the `liferay/liferay-portal` and `liferay/liferay-portal-ee` repositories as a way to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "github_base_url = 'https://api.github.com'\n",
    "\n",
    "def is_repository_accessible(reviewer_url):\n",
    "    print('Validating OAuth token against %s' % reviewer_url)\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'python checklpp.py',\n",
    "        'authorization': 'token %s' % github_oauth_token\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s' % reviewer_url\n",
    "    \n",
    "    r = requests.get(github_base_url + api_path, headers=headers)\n",
    "    \n",
    "    return r.status_code == 200\n",
    "\n",
    "assert(is_repository_accessible('liferay/liferay-portal'))\n",
    "assert(is_repository_accessible('liferay/liferay-portal-ee'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Join Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We currently have one data set (a list of JIRA tickets that are in review), and our next step is to think about how we can answer our actual question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Now that we've identified which LPP tickets are currently in review, our next step is to identify pull requests that are stuck in review. So, that leaves us with the following question: how should we implement this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of Query Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will happen from here is you ask the database to provide an explanation of what it's doing for your query, you work some magic to make this explanation look better (add database indices, modify the query), and you commit those changes to the codebase.\n",
    "\n",
    "* [Query plan](https://en.wikipedia.org/wiki/Query_plan)\n",
    "\n",
    "Our definition of \"better\" comes from interpreting certain aspects of the query plan as necessarily worse than what we expected. Our expectations often come from definitions that are provided by various database vendors on the ideal query plan.\n",
    "\n",
    "* [MySQL explain plan](https://dev.mysql.com/doc/refman/5.6/en/explain-output.html)\n",
    "* [Oracle explain plan](https://docs.oracle.com/database/121/TGSQL/tgsql_interp.htm)\n",
    "* [PostgreSQL explain plan](http://www.postgresql.org/docs/9.4/static/using-explain.html)\n",
    "* [SQL Server explain plan](https://technet.microsoft.com/en-us/library/ms178071%28v=sql.105%29.aspx)\n",
    "\n",
    "When you run into a slow query and you ask for it to provide an explanation of its approach, the output you receive is the query plan derived by the query optimizer.\n",
    "\n",
    "* [Query optimization](https://en.wikipedia.org/wiki/Query_optimization)\n",
    "\n",
    "Query optimization plans essentially evaluate various ways of correctly answering the query described by the SQL statement while also minimizing the cost of loading the query. For a database, this notion of \"cost\" can be summarized as use of memory (in particular, pages getting swapped in and out of active memory), disk I/O, and network I/O.\n",
    "\n",
    "* [Relative time cost of computer operations](http://norvig.com/21-days.html#answers)\n",
    "\n",
    "The result of this optimized plan particularly easy to understand in Microsoft SQL Server, where it gives you a graphical representation of its query plan, and it estimates what percentage of the time spent processing the query will be spent on that specific aspect of it.\n",
    "\n",
    "* [SQL Server query execution plans](http://www.sqlshack.com/sql-server-query-execution-plans-understanding-reading-plans/)\n",
    "* [SQL Server graphical plan icons](https://technet.microsoft.com/en-us/library/ms175913%28v=sql.105%29.aspx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goals of Join Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking a step back, what are we doing is that we need to derive new information from two separate data sources. If these two data sources were both tables, then if these tables have columns that refer to the same abstract concept (or perhaps a third \"mapping table\" that describes that abstract concept), then we would join these two \n",
    "\n",
    "* [Khan Academy: Joining Related Tables](https://www.khanacademy.org/computing/computer-programming/sql/relational-queries-in-sql/p/joining-related-tables)\n",
    "\n",
    "We've identified all of the JIRA issues that are in review. Presumably, we want to then combine this with GitHub data. What algorithm will we implement to achieve this? It turns out there are three basic strategies for computing a join: a nested loop join, a hash join, and a sort-merge join.\n",
    "\n",
    "* [Join methods and subqueries](http://www.orafaq.com/tuningguide/join%20methods.html)\n",
    "\n",
    "Sort-merge deserves its own discussion (which we'll do in the next tutorial), and it will make more sense after we're finished than before we've finished. So we'll look first at how a nested loop join and a hash join relate to our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Loop Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple solution to this problem would be to iterate over each of our pull requests and then check each JIRA ticket to see if the JIRA ticket references the pull request. So for every GitHub pull request, you would check every JIRA ticket. This strategy is known as a **nested loop join**.\n",
    "\n",
    "A nested loop join is a join where the query optimizer decides the best way to accomplish the join is to designate one table as the \"outer table\" (no connection to the notion of an outer join) and the other table as the \"inner table\", structured in much the same way as a for loop.\n",
    "\n",
    "* [Nested loop join](https://en.wikipedia.org/wiki/Nested_loop_join)\n",
    "\n",
    "How fast is a nested loop join? Let $m$ be the size of the JIRA table and $n$ be the size of the GitHub table. Regardless of which table you choose for the outer table, a nested loop join would have an expected runtime and a worst-case runtime of $O(m \\cdot n)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hash Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another solution to this problem would be to first load all the pull requests into a lookup data structure. From there, iterate over the JIRA tickets and match the pull requests to the hash table. So for every JIRA ticket, you would perform a number of lookups against our data structure not based on either tables size (so, effectively a constant). This strategy is known as a **hash join**.\n",
    "\n",
    "A hash join is a join where the query optimizer decides the best way to accomplish the join is to build a lookup table on the smaller table (because it's more likely that a smaller table can fit into memory), with a popular choice being a hash table, and then iterate over the larger table.\n",
    "\n",
    "* [Hash join](https://en.wikipedia.org/wiki/Hash_join)\n",
    "\n",
    "How fast is a hash join? Let $m$ be the size of the JIRA table and $n$ be the size of the GitHub table. Note that hash tables have an $O(1)$ expected access and insertion time, but an $O(n)$ worst case access and insertion time (due to hash collisions, table resizes). So in practice, we have an expected runtime that is $O(m) + O(n)$ and a worst-case runtime that is $O(m \\cdot n) + O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub, Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a valid login, we can proceed with our question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "As noted previously, before we can answer the question of whether anything is *stuck in review*, we will need to answer the broader question of what is *in review*. Now that we know what's in review in JIRA, we also want to what's in review on GitHub. Then, we will want to join these two tables together.\n",
    "\n",
    "Between a nested loop join and a hash join, the hash join has the better expected runtime. So how can we perform a hash join? We need to create a lookup data structure containing all the pull requests using some key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve GitHub Pull Requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, given the number of unique users repositories that are involved in reviewing Liferay pull requests, it's not practical to make an API call against every user and every repository. Instead, we'll want to do this on-demand based on which pull requests we know exist.\n",
    "\n",
    "If you visit `/repos/USERNAME/REPOSITORY/pulls` without a pull ID, GitHub will return all currently open pull requests. The approach below uses this API in an attempt to reduce the number of API requests to GitHub, because this will fetch all pull requests that are open in a single request.\n",
    "\n",
    "Following our principle of keeping any data we retrieve, we save all of these pull requests, and then we request any additional pull requests that are closed and save those as well. Because our end goal is a hash join, we'll load this table as a map or dictionary where the key is the GitHub URL, because that's what will be present in JIRA fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_pull_requests(reviewer_url, pull_request_ids=[]):\n",
    "    print('Checking pull requests waiting on %s' % reviewer_url)\n",
    "\n",
    "    headers = {\n",
    "        'user-agent': 'python checklpp.py',\n",
    "        'authorization': 'token %s' % github_oauth_token\n",
    "    }\n",
    "\n",
    "    api_path = '/repos/%s/pulls' % reviewer_url\n",
    "\n",
    "    r = requests.get(github_base_url + api_path, headers=headers)\n",
    "\n",
    "    if r.status_code != 200:\n",
    "        return {}\n",
    "\n",
    "    new_pull_requests_list = r.json()\n",
    "    new_pull_requests = {\n",
    "        pull_request['html_url']: pull_request\n",
    "            for pull_request in new_pull_requests_list\n",
    "    }\n",
    "\n",
    "    for pull_request_id in pull_request_ids:\n",
    "        github_url = 'https://github.com/%s/pull/%s' % (reviewer_url, pull_request_id)\n",
    "\n",
    "        if github_url in new_pull_requests:\n",
    "            continue\n",
    "\n",
    "        api_path = '/repos/%s/pulls/%s' % (reviewer_url, pull_request_id)\n",
    "\n",
    "        r = requests.get(github_base_url + api_path, headers=headers)\n",
    "\n",
    "        if r.status_code != 200:\n",
    "            continue\n",
    "\n",
    "        new_pull_requests[github_url] = r.json()\n",
    "\n",
    "    return new_pull_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, we'll take a look at all the pull requests open against `liferay/liferay-portal-ee`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_open_backports():\n",
    "    open_backports = load_raw_dict('open_backports')\n",
    "\n",
    "    if open_backports is not None:\n",
    "        print('Loaded cached open backports')\n",
    "        return open_backports\n",
    "        \n",
    "    open_backports = retrieve_pull_requests('liferay/liferay-portal-ee')\n",
    "    open_backports = save_raw_dict('open_backport_pulls', open_backports)\n",
    "    return open_backports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    open_backports = get_open_backports()\n",
    "else:\n",
    "    open_backports = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GHPullRequest = namedtuple(\n",
    "    'GHPullRequest',\n",
    "    ['submitter', 'pull_id', 'branch', 'created_at', 'updated_at', 'closed_at', 'state', 'github_url']\n",
    ")\n",
    "\n",
    "def get_github_tuple(pull_request):\n",
    "    pull_id = '%s/%s#%d' % (\n",
    "        pull_request['base']['user']['login'],\n",
    "        pull_request['base']['repo']['name'],\n",
    "        pull_request['number']\n",
    "    )\n",
    "    \n",
    "    return GHPullRequest(\n",
    "        submitter=pull_request['user']['login'],\n",
    "        pull_id=pull_id,\n",
    "        branch=pull_request['base']['ref'],\n",
    "        created_at=pull_request['created_at'],\n",
    "        updated_at=pull_request['updated_at'],\n",
    "        closed_at=pull_request['closed_at'],\n",
    "        state=pull_request['state'],\n",
    "        github_url=pull_request['html_url']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_github_tuple(pull_request) for pull_request in open_backports.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Mapping Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a function that can retrieve GitHub pull request metadata, you could say that in addition to having a JIRA table and a GitHub table. However, how can we join the two together?\n",
    "\n",
    "The answer is a strategy that you see implemented in Liferay Service Builder. This strategy is known as a mapping table, named for how it explicitly creates many-to-many mappings.\n",
    "\n",
    "* [Understanding Mapping Tables](https://stackoverflow.com/questions/6453462/mysql-understanding-mapping-tables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping Table Extensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's assume you have two tables, JIRA and GitHub You can use a mapping table in order to document the primary keys of JIRA and how they relate to the primary keys on GitHub.\n",
    "\n",
    "What kinds of question can you answer with a mapping table? Simplistically, you could fetch all JIRA tickets corresponding to a specific GitHub pull request and all GitHub pull requests corresponding to a specific JIRA ticket. If you perform a three-way join, you could get all JIRA tickets and all GitHub pull requests that reference each other.\n",
    "\n",
    "To answer these questions, you have to think more carefully about how to optimize these queries. A simple solution that works on smaller data sets is to load the entire mapping table into memory and perform hash joins on both sides.\n",
    "\n",
    "However, database vendors have found that you can technically do better. There is a logical extension of a mapping table that brings together more than two keys, and this extension is referred to as a star schema. These are actually common in data warehouses, and databases can be configured (usually through the use of specialized database index types) to drastically improve query cost plans against star schemas.\n",
    "\n",
    "* [Star schema](https://en.wikipedia.org/wiki/Star_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data from GitHub, Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that we have our lookup data structure, what do we need to answer our question?\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "Well, while we have a lookup data structure, it turns out we don't actually have a GitHub table yet, only a subset of the GitHub table corresponding to pull requests sent against `liferay/liferay-portal-ee`! So our next step is to generate the relevant portion of our GitHub table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify GitHub Pulls Related to JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So while we have a utility method that allows us to retrieve GitHub pull request histories, we don't actually have most of that information. So how do we identify which ones to retrieve?\n",
    "\n",
    "Our first step is to iterate over each JIRA ticket and extract the pull requests contained in the ticket. Once we have that information, we'll be able to use our lookup data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_jira_pull_request_urls(jira_issues):\n",
    "    issues_by_request = defaultdict(set)\n",
    "    requests_by_issue = defaultdict(set)\n",
    "    requests_by_reviewer = defaultdict(set)\n",
    "\n",
    "    for jira_key, jira_issue in jira_issues.items():\n",
    "        for value in jira_issue['fields'].values():\n",
    "            if not isinstance(value, six.string_types):\n",
    "                continue\n",
    "\n",
    "            for github_url in re.findall('https://github.com/[^\\s]*/pull/[\\d]+', value):\n",
    "                requests_by_issue[jira_key].add(github_url)\n",
    "                issues_by_request[github_url].add(jira_key)\n",
    "\n",
    "    return issues_by_request, requests_by_issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's pass the data the results of our previous JIRA search to this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jira_pull_request_urls(jql):\n",
    "    base_name_1 = get_jql_hashed_name('issues_by_request', jql)\n",
    "    base_name_2 = get_jql_hashed_name('requests_by_issue', jql)\n",
    "    \n",
    "    issues_by_request = load_raw_dict(base_name_1)\n",
    "    requests_by_issue = load_raw_dict(base_name_2)\n",
    "\n",
    "    if issues_by_request is not None and requests_by_issue is not None:\n",
    "        print('Loaded cached JIRA to GitHub mapping')\n",
    "        return issues_by_request, requests_by_issue\n",
    "    \n",
    "    jira_issues = get_jira_issues(jql)\n",
    "    issues_by_request, requests_by_issue = extract_jira_pull_request_urls(jira_issues)\n",
    "\n",
    "    issues_by_request = save_raw_dict(base_name_1, issues_by_request)\n",
    "    requests_by_issue = save_raw_dict(base_name_2, requests_by_issue)\n",
    "    \n",
    "    return issues_by_request, requests_by_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    issues_by_request, requests_by_issue = get_jira_pull_request_urls(in_review_jql)\n",
    "else:\n",
    "    issues_by_request = {}\n",
    "    requests_by_issue = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JIRAGitHubMapping = namedtuple('JIRAGitHubMapping', ['jira_key', 'github_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    JIRAGitHubMapping(jira_key=jira_key, github_url=github_url)\n",
    "        for jira_key, github_urls in requests_by_issue.items()\n",
    "            for github_url in github_urls\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it turns out that what we've done is equivalent to building a mapping table between the JIRA tickets (represented with their issue key) and the GitHub pull requests (represented with their URL)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieve GitHub Pulls Related to JIRA Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll want to fetch all the metadata associated with all those pull requests. We'll also want a quick way to separate the open/active pull requests from the closed/inactive pull requests so that we don't have to re-derive that metadata later on when we attempt to identify stuck pull requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def retrieve_related_pull_requests(issues_by_request):\n",
    "    requests_by_reviewer = defaultdict(set)\n",
    "    \n",
    "    for github_url in issues_by_request.keys():\n",
    "        reviewer_url = github_url[19:github_url.rfind('/pull/')]\n",
    "        requests_by_reviewer[reviewer_url].add(github_url[github_url.rfind('/')+1:])\n",
    "\n",
    "    related_pull_requests = {}\n",
    "\n",
    "    for reviewer_url, pull_request_ids in sorted(requests_by_reviewer.items()):\n",
    "        new_pull_requests = retrieve_pull_requests(reviewer_url, pull_request_ids)\n",
    "        related_pull_requests.update(new_pull_requests)\n",
    "\n",
    "    return related_pull_requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go ahead and use our mapping tables to populate the subset of the GitHub table corresponding to only (1) currently active pull requests, or (2) closed pull requests corresponding to a Liferay Portal Patch ticket that is currently in review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_related_pull_requests(jql):\n",
    "    base_name = get_jql_hashed_name('related_pull_requests', jql)\n",
    "    \n",
    "    related_pull_requests = load_raw_dict(base_name)\n",
    "\n",
    "    if related_pull_requests is not None:\n",
    "        print('Loaded cached pull request metadata')\n",
    "        return related_pull_requests\n",
    "\n",
    "    issues_by_request, requests_by_issue = get_jira_pull_request_urls(jql)\n",
    "\n",
    "    related_pull_requests = retrieve_related_pull_requests(issues_by_request)\n",
    "    related_pull_requests = save_raw_dict(base_name, related_pull_requests)\n",
    "    \n",
    "    return related_pull_requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    related_pull_requests = get_related_pull_requests(in_review_jql)\n",
    "else:\n",
    "    related_pull_requests = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([get_github_tuple(pull_request) for pull_request in related_pull_requests.values()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen in the result of executing the previous code cells, a lot of what we're doing can be conceptualized as building tables that satisfy our search parameters. In the case of JIRA, it's all issues matching our query string. In the case of GitHub, it's all open pull requests alongside all of the pull requests that are tied to an LPP ticket.\n",
    "\n",
    "Just as lists of results can be conceptualized as a table, so too are things that work with regular SQL. More explicitly, whenever you're working with a database and you execute SQL, the result can be understood to be another table, whether this table consists of a single value, a single row, or multiple rows.\n",
    "\n",
    "* [Relational algebra](https://en.wikipedia.org/wiki/Relational_algebra)\n",
    "\n",
    "At a conceptual level, running database queries is taking existing tables and generating (or deriving) new tables that better fit the question you are trying to answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persisting Derived Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last point is worth emphasizing, as it comes up in upgrade performance tuning. In short, running database queries always generates new tables. More explicitly, the database will have gone through all of the effort required to create a table when executing your query, skipping only the persistence step.\n",
    "\n",
    "* [The difference between subqueries and derived tables in SQL](https://www.xaprb.com/blog/2005/09/26/sql-subqueries-and-derived-tables/)\n",
    "\n",
    "Looking at this a little differently, imagine that we decided to remove the code that invoked the `load_raw_dict` and `save_raw_dict` functions. Would this have noticeably improved the performance of our code? Not really, because the retrieval from JIRA is slower than writing the data to disk. That's equivalent to the cost difference between persisting the data and not persisting it.\n",
    "\n",
    "Having the derived table be saved as an permanent table also allows you to add metadata, such as indices, that will improve the performance on repeated queries. So, in cases where you are operating on different subsets of a larger subset of the data, and this larger subset is expensive to compute, it's actually very sensible to save your derived table to reduce query execution time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform JIRA-GitHub Join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have raw data from both JIRA and GitHub. How can we proceed in answering our original question?\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "One way that we can define \"stuck in review\" is an LPP ticket that has a GitHub pull request that has remained open for a long time. Another way that we can define \"stuck in review\" is an LPP ticket that has no open GitHub pull requests, but is still in review even after the last pull request closed.\n",
    "\n",
    "Both of these require us to perform a join between our two tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open LPP Pull Request Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of these questions require us to join the two tables together, and then filter the end result. What we treat as idle time is different depending on whether there are zero open pull requests or at least one open pull request.\n",
    "\n",
    "Our job would be easier if we knew how many pull requests that were still open for each JIRA ticket, which we could express with the following SQL.\n",
    "\n",
    "```\n",
    "SELECT Jira.ticket_key AS jira_key,\n",
    "       count(*)        AS open_count\n",
    "FROM   Jira\n",
    "       LEFT OUTER JOIN Jira_GitHub\n",
    "                    ON Jira.ticket_key = Jira_GitHub.jira_key\n",
    "       INNER JOIN GitHub\n",
    "               ON Jira_GitHub.github_url = GitHub.github_url\n",
    "WHERE  GitHub.state = 'open'\n",
    "GROUP  BY Jira.ticket_key\n",
    "```\n",
    "\n",
    "So we're going to persist the result of this computation to make future computation easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jira_github_join(jql):\n",
    "    base_name = get_jql_hashed_name('jira_github_join', jql)\n",
    "    \n",
    "    jira_github_join = load_raw_dict(base_name)\n",
    "\n",
    "    if jira_github_join is not None:\n",
    "        print('Loaded cached JIRA-GitHub join result')\n",
    "        return jira_github_join\n",
    "\n",
    "    jira_issues = get_jira_issues(jql)\n",
    "    issues_by_request, requests_by_issue = get_jira_pull_request_urls(jql)\n",
    "    related_pull_requests = get_related_pull_requests(jql)\n",
    "\n",
    "    jira_github_join = {\n",
    "        jira_key: {\n",
    "            'jira': jira_issues[jira_key],\n",
    "            'github': [related_pull_requests[github_url] for github_url in github_urls]\n",
    "        }\n",
    "        for jira_key, github_urls in requests_by_issue.items()\n",
    "            if github_url in related_pull_requests\n",
    "    }\n",
    "    \n",
    "    jira_github_join = save_raw_dict(base_name, jira_github_join)\n",
    "    \n",
    "    return jira_github_join\n",
    "\n",
    "def get_github_open_count(jql):\n",
    "    base_name = get_jql_hashed_name('github_open_count', jql)\n",
    "    \n",
    "    github_open_count = load_raw_dict(base_name)\n",
    "\n",
    "    if github_open_count is not None:\n",
    "        print('Loaded cached JIRA-GitHub join count result')\n",
    "        return github_open_count\n",
    "\n",
    "    jira_github_join = get_jira_github_join(jql)\n",
    "\n",
    "    github_open_count = {\n",
    "        jira_key: len([pull_request for pull_request in join_result['github'] if pull_request['state'] == 'open'])\n",
    "            for jira_key, join_result in jira_github_join.items()\n",
    "    }\n",
    "    \n",
    "    github_open_count = save_raw_dict(base_name, github_open_count)\n",
    "    \n",
    "    return github_open_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    github_open_count = get_github_open_count(in_review_jql)\n",
    "else:\n",
    "    github_open_count = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GitHubOpenCount = namedtuple('GitHubOpenCount', ['jira_key', 'open_count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    GitHubOpenCount(jira_key=jira_key, open_count=open_count)\n",
    "        for jira_key, open_count in github_open_count.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck in Review: At Least One Open Pull"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now that we have this table, we can look for the creation time on any open pull requests for any ticket with at least one open pull request.\n",
    "\n",
    "```\n",
    "SELECT GitHubOpenCount.jira_key,\n",
    "       GitHub.*\n",
    "FROM   GitHubOpenCount\n",
    "       INNER JOIN Jira_GitHub\n",
    "               ON Jira.ticket_key = Jira_GitHub.jira_key\n",
    "       INNER JOIN GitHub\n",
    "               ON Jira_GitHub.github_url = GitHub.github_url\n",
    "WHERE  GitHubOpenCount.open_count > 0\n",
    "       AND GitHub.state = 'open'\n",
    "GROUP  BY GitHubOpenCount.jira_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_github_idle_tickets(jql):\n",
    "    base_name = get_jql_hashed_name('github_idle_tickets', jql)\n",
    "    \n",
    "    github_idle_tickets = load_raw_dict(base_name)\n",
    "\n",
    "    if github_idle_tickets is not None:\n",
    "        print('Loaded cached list of tickets idle on GitHub')\n",
    "        return github_idle_tickets\n",
    "\n",
    "    github_open_count = get_github_open_count(jql)    \n",
    "    jira_github_join = get_jira_github_join(jql)\n",
    "    \n",
    "    github_idle_tickets = {\n",
    "        jira_key: {\n",
    "            'jira': join_result['jira'],\n",
    "            'github': [pull_request for pull_request in join_result['github'] if pull_request['state'] == 'open']\n",
    "        }\n",
    "        for jira_key, join_result in jira_github_join.items() if github_open_count[jira_key] > 0\n",
    "    }\n",
    "\n",
    "    github_idle_tickets = save_raw_dict(base_name, github_idle_tickets)\n",
    "    \n",
    "    return github_idle_tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    github_idle_tickets = get_github_idle_tickets(in_review_jql)\n",
    "else:\n",
    "    github_idle_tickets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "JiraGitHubLookup = namedtuple(\n",
    "    'JiraGitHubLookup',\n",
    "    list(JIRAIssue._fields) + list(GHPullRequest._fields)\n",
    ")\n",
    "\n",
    "def get_jira_github_tuple(jira_key, jira_issue, pull_request):\n",
    "    jira_tuple = get_jira_tuple(jira_issue)\n",
    "    github_tuple = get_github_tuple(pull_request)\n",
    "\n",
    "    combined_columns = jira_tuple._asdict()\n",
    "    combined_columns.update(github_tuple._asdict())\n",
    "    \n",
    "    new_tuple = JiraGitHubLookup(**combined_columns)\n",
    "\n",
    "    return new_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    get_jira_github_tuple(jira_key, join_result['jira'], pull_request)\n",
    "        for jira_key, join_result in github_idle_tickets.items()\n",
    "            for pull_request in join_result['github']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck In Review: No Open Pulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also look for the updated time on the last closed pull request for any ticket with zero open pull requests.\n",
    "\n",
    "```\n",
    "SELECT GitHubOpenCount.jira_key as jira_key,\n",
    "       max(GitHub.closed_at) AS closed_at\n",
    "FROM   GitHubOpenCount\n",
    "       INNER JOIN Jira_GitHub\n",
    "               ON Jira.ticket_key = Jira_GitHub.jira_key\n",
    "       INNER JOIN GitHub\n",
    "               ON Jira_GitHub.github_url = GitHub.github_url\n",
    "WHERE  GitHubOpenCount.open_count = 0\n",
    "       AND GitHub.state = 'closed'\n",
    "GROUP  BY GitHubOpenCount.jira_key\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_jira_idle_tickets(jql):\n",
    "    base_name = get_jql_hashed_name('jira_idle_tickets', jql)\n",
    "    \n",
    "    jira_idle_tickets = load_raw_dict(base_name)\n",
    "\n",
    "    if jira_idle_tickets is not None:\n",
    "        print('Loaded cached list of tickets idle on JIRA')\n",
    "        return jira_idle_tickets\n",
    "\n",
    "    github_open_count = get_github_open_count(jql)    \n",
    "    jira_github_join = get_jira_github_join(jql)\n",
    "    \n",
    "    github_closed_pulls = {\n",
    "        key: {\n",
    "            'jira': join_result['jira'],\n",
    "            'github': [\n",
    "                pull_request\n",
    "                    for pull_request in join_result['github'] if pull_request['state'] == 'closed'\n",
    "            ]\n",
    "        }\n",
    "        for key, join_result in jira_github_join.items() if github_open_count[key] == 0\n",
    "    }\n",
    "    \n",
    "    jira_idle_tickets = {\n",
    "        key: {\n",
    "            'jira': join_result['jira'],\n",
    "            'github': max(join_result['github'], key=lambda x: x['closed_at'])\n",
    "        }\n",
    "        for key, join_result in github_closed_pulls.items()\n",
    "    }\n",
    "\n",
    "    jira_idle_tickets = save_raw_dict(base_name, jira_idle_tickets)\n",
    "    \n",
    "    return jira_idle_tickets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    jira_idle_tickets = get_jira_idle_tickets(in_review_jql)\n",
    "else:\n",
    "    jira_idle_tickets = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    get_jira_github_tuple(jira_key, join_result['jira'], join_result['github'])\n",
    "        for jira_key, join_result in jira_idle_tickets.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Computed Fields"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list of LPP tickets that have at least one open GitHub pull request. We also have a list of LPP tickets that have no open GitHub pull requests but are still in review, and the last closed pull request associated with that LPP ticket. Let's come back to our question.\n",
    "\n",
    "<b style=\"color:green\">Are there Liferay Portal Patches (LPP) tickets that are currently stuck in review?</b>\n",
    "\n",
    "While we could look at the dates to determine how long everything has been open, it'd be nice if that value was displayed for us in a different way. That would make it much easier to see, at a glance, what's still idle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add Idle Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All we're doing now is adding a computed value to our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_time_delta_as_days(time_delta):\n",
    "    return float(time_delta.days) + float(time_delta.seconds) / (60 * 60 * 24)\n",
    "\n",
    "new_fields = list(JiraGitHubLookup._fields) + ['open_time_days', 'idle_time_days']\n",
    "removed_fields = ['created_at', 'updated_at', 'closed_at']\n",
    "\n",
    "for removed_field in removed_fields:\n",
    "    new_fields.remove(removed_field)\n",
    "\n",
    "JiraGitHubLookupIdleTime = namedtuple('JiraGitHubLookupIdleTime', new_fields)\n",
    "\n",
    "def get_jira_github_idle_time_tuple(jira_key, jira_issue, pull_request):\n",
    "    old_tuple = get_jira_github_tuple(jira_key, jira_issue, pull_request)\n",
    "    old_values = old_tuple._asdict()\n",
    "\n",
    "    for removed_field in removed_fields:\n",
    "        del old_values[removed_field]\n",
    "    \n",
    "    created_at = dateparser.parse(pull_request['created_at'])\n",
    "    updated_at = dateparser.parse(pull_request['updated_at'])\n",
    "\n",
    "    closed_at = pull_request['closed_at']\n",
    "    \n",
    "    if closed_at is None:\n",
    "        open_time_days = get_time_delta_as_days(now - created_at)\n",
    "        idle_time_days = get_time_delta_as_days(now - updated_at)\n",
    "    else:\n",
    "        closed_at = dateparser.parse(pull_request['closed_at'])\n",
    "\n",
    "        open_time_days = None\n",
    "        idle_time_days = get_time_delta_as_days(now - closed_at)\n",
    "    \n",
    "    new_tuple = JiraGitHubLookupIdleTime(\n",
    "        open_time_days=open_time_days,\n",
    "        idle_time_days=idle_time_days,\n",
    "        **old_values\n",
    "    )\n",
    "\n",
    "    return new_tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck in Review: At Least One Open Pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    get_jira_github_idle_time_tuple(jira_key, join_result['jira'], pull_request)\n",
    "        for jira_key, join_result in github_idle_tickets.items()\n",
    "            for pull_request in join_result['github']\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stuck in Review: No Open Pulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([\n",
    "    get_jira_github_idle_time_tuple(jira_key, join_result['jira'], join_result['github'])\n",
    "        for jira_key, join_result in jira_idle_tickets.items()\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the Results for Grow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have all the data, it's time to export it so that something outside of this notebook can use it. For simplicity, we'll export the data as Javascript variables, which can then be accessed by other Javascript on the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    github_idle_tickets_list = [\n",
    "        get_jira_github_idle_time_tuple(jira_key, join_result['jira'], pull_request)._asdict()\n",
    "            for jira_key, join_result in github_idle_tickets.items()\n",
    "                for pull_request in join_result['github']\n",
    "    ]\n",
    "    \n",
    "    jira_idle_tickets_list = [\n",
    "        get_jira_github_idle_time_tuple(jira_key, join_result['jira'], join_result['github'])._asdict()\n",
    "            for jira_key, join_result in jira_idle_tickets.items()\n",
    "    ]\n",
    "    \n",
    "    with open('%s_idle_ticket_data.json' % today.isoformat(), 'w') as outfile:\n",
    "        idle_ticket_data = {\n",
    "            'lastUpdated': now.isoformat(),\n",
    "            'githubIdleTicketsList': github_idle_tickets_list,\n",
    "            'jiraIdleTicketsList': jira_idle_tickets_list\n",
    "        }\n",
    "        \n",
    "        json.dump(idle_ticket_data, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Notebook to Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell will use `jupyter nbconvert` to build an `checklpp.py` which runs the script outlined in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "var script_file = 'checklpp.py';\n",
    "\n",
    "var notebook_name = window.document.getElementById('notebook_name').innerHTML;\n",
    "var nbconvert_command = 'jupyter nbconvert --stdout --to script ' + notebook_name;\n",
    "\n",
    "var grep_command = \"grep -v '^#' | grep -v -F get_ipython | sed '/^$/N;/^\\\\n$/D'\";\n",
    "var command = '!' + nbconvert_command + ' | ' + grep_command + ' > ' + script_file;\n",
    "\n",
    "if (Jupyter.notebook.kernel) {\n",
    "    Jupyter.notebook.kernel.execute(command);\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
